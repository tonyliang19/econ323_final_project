{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a028c846-e33d-4839-b554-0ce4c78f4a1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Group Members:** Tony Liang (004), Wanxin luo (003), Xuan Chen (004)\n",
    "\n",
    "**Student Numbers:** 39356993, 33432808, 15734643\n",
    "\n",
    "\n",
    "ECON 323 Quantitative Economic Modelling with Data Science Applications UBC 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fb6f8-bfdd-4817-a220-599c2a2d2d5d",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ccee084-e0f4-4a04-a64a-b4acf1a57a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee480b-3072-4d76-b7aa-c32eb9b9a8f6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project, we aim to **explore the impact of environmental factors on housing prices** using the [Boston Housing dataset](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data.). This dataset contains information on various attributes such as crime rate, average number of rooms, accessibility to highways, and more, which are hypothesized to influence housing prices. The project will involve several parts, including data cleaning, visualization, and model building. Our objective is to conduct exploratory data analysis (EDA) and then build a hedonic regression model with multiple inputs. We will utilize various Python techniques learned in this course to explore the real world data and solve economic questions.\n",
    "\n",
    "By analyzing the data, we aim to answer economic questions related to the housing market and explore the real-world application of Python techniques. It is important to note that the dataset has its limitations as it was collected almost 50 years ago, but it still provides an excellent opportunity for us to apply our Python skills and gain insights of housing market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f33ad8-1d26-4fee-bef7-da08aec438f9",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e018202-c9b9-4f9c-ba64-ba7f8167765a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston, MA. The following describes the dataset columns:\n",
    "\n",
    "- `CRIM` - per capita crime rate by town\n",
    "- `ZN` - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- `INDUS` - proportion of non-retail business acres per town.\n",
    "- `CHAS` - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- `NOX` - nitric oxides concentration (parts per 10 million)\n",
    "- `RM` - average number of rooms per dwelling\n",
    "- `AGE` - proportion of owner-occupied units built prior to 1940\n",
    "- `DIS` - weighted distances to five Boston employment centres\n",
    "- `RAD` - index of accessibility to radial highways\n",
    "- `TAX` - full-value property-tax rate per $10,000$\n",
    "- `PTRATIO` - pupil-teacher ratio by town\n",
    "- $B - 1000(Bk - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "- `LSTAT` - $%$ lower status of the population\n",
    "- `MEDV` - Median value of owner-occupied homes in $1000$'s\n",
    "\n",
    "The dataset is derived from https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5dc5e-e263-48ce-84e7-250773a1e413",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89eb50-575a-4ad2-81d6-cdb95409b0c0",
   "metadata": {},
   "source": [
    "This report strives to be trustworthy using the following steps: \n",
    "\n",
    "1. [Data cleaning](#data-cleaning)\n",
    "2. [Thorough EDA](#eda)\n",
    "3. [Building multiple linear regression model](#model-fitting)\n",
    "\n",
    "**Note**: this could be subjected to changes later after feedback from the ECON323 Instrutor's team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28983d56-2612-4c74-9436-f007e440bf20",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "For the data cleaning step, we will check and handle the missing values in the dataset. We will also identify categorical and continuous variables. For instance, the `CHAS` variable is a dummy variable indicating whether the tract bounds the Charles River or not, and is encoded as 0 or 1. Moreover, perform any special treatments toward outliers depending on the method that will be carried in the [model fitting phase](#model-fitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880d72c-e5ed-4150-af2d-85ec545002f7",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "During the EDA phrase, we will conduct a thorough examination of the Boston Housing dataset. One of the key steps is to generate a correlation matrix, which can help us identify any potential issues related to multicollinearity between the independent variables. In addition, we will use side-by-side box plots to visualize the distributions of the continuous variables and detect any potential outliers or anomalies. Moreover, we will leverage other data visualization techniques, such as scatter plots and histograms, to better understand the relationships between the variables and explore potential trends or patterns in the data. Overall, the goal of EDA is to gain insights into the data and inform our subsequent modelling steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c4af49-52be-4db5-b16d-632b4767f616",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "In the model fitting phase, we will split the Boston Housing dataset into training and testing sets. We will then use the training set to select the relevant variables and build our final multiple linear regression model. The selection process can involve various techniques, such as stepwise regression or regularization, depending on the specific requirements of the project. Once we have the final model, we will use the testing set to evaluate its performance in terms of mean squared error (`MSE`). The goal is to ensure that the model can generalize well to new, unseen data and make accurate predictions. \n",
    "\n",
    "To further explore effects of using different methods of regression, we are going to fit multiple models to using similar metrics accross these to compare best fit of model, i.e. Bayesian Information Criterion (BIC) and Adjusted $R^2$ for inference (how well our model explains the effects of the explanatory variables is of the variable of interes); Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for prediction purposes models. Our ideal approach to test this is to fit the following models:\n",
    "1. Ordinary Least Squares (OLS) as baseline \n",
    "2. OLS with L2-norm regularization (Ridge Regression)\n",
    "3. OLS with L1-norm regularization (Least Absolute Shrinkage and Selection Operator (LASSO Regression) )\n",
    "4. Decision Tree Regression\n",
    "5. Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25328620-3308-440b-8c05-da254cd7e53f",
   "metadata": {},
   "source": [
    "#### Load and Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50397b-020e-4fec-8abc-a40fd81a3961",
   "metadata": {},
   "source": [
    "To begin, we will have a common block to define function to split the data into train and test data for further modelling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47265083-9b2b-4a91-8b2f-ece6ec135331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load and split data\n",
    "\n",
    "# loads data from path, and specifies proportion of train data, with 1 - proportion of test data\n",
    "# and target is the variable of interest (your y), then returns train and test data\n",
    "# proportion is default to 0.5, and target default to None \n",
    "def split_data(data_path, proportion=0.5, target=None, random_state=123):\n",
    "    \"\"\"\n",
    "    Loads data from path, and specifies proportion of train data, with 1 - proportion of test data\n",
    "    and target is the variable of interest (your y), then returns train and test data\n",
    "    proportion is default to 0.5, and target default to None.\n",
    "    \n",
    "    Optional argument:\n",
    "    random_state = 123 (default), change to other number of your choice to assert reproducibility\n",
    "    \"\"\"\n",
    "    # load the data\n",
    "    data = pd.read_csv(data_path)\n",
    "    # drop nas\n",
    "    data = data.dropna()\n",
    "    # inner function to split data into train and test portion\n",
    "    def train_test_split(data, proportion):\n",
    "        train = data.sample(frac = proportion, random_state=random_state)\n",
    "        test = data.drop(train.index)\n",
    "        # rest and remove index of both\n",
    "        train = train.reset_index().drop(columns=[\"index\"])\n",
    "        test = test.reset_index().drop(columns=[\"index\"])\n",
    "        # asserting dimension matches (i.e. number of rows)\n",
    "        assert train.shape[0] + test.shape[0] == data.shape[0]\n",
    "        return train, test\n",
    "    # split the data into train and test\n",
    "    train, test = train_test_split(data, proportion)\n",
    "    # further split train data to X and y\n",
    "    def split_X_y(data):\n",
    "        X = data.drop(columns=[target])\n",
    "        y = data[target]\n",
    "        return X, y\n",
    "    X_train, y_train = split_X_y(train)\n",
    "    # split test data to X and y\n",
    "    X_test, y_test = split_X_y(test)\n",
    "    # check dimension again\n",
    "    assert X_train.shape[0] + X_test.shape[0] == data.shape[0]\n",
    "    assert X_train.shape[1] and X_test.shape[1] == 13\n",
    "    assert y_train.shape[0] == X_train.shape[0] and y_test.shape[0] == X_test.shape[0]\n",
    "    # return the objects needed\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "68b376a9-b0a7-4027-803c-c295306f42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to find data\n",
    "path = \"data/boston_housing_data.csv\"\n",
    "# Splits the data into X and y train and test portions\n",
    "X_train, X_test, y_train, y_test = split_data(path, proportion = 0.75, target = \"MEDV\", random_state=20230325)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73d9cf6-4e29-49b9-a143-9b5f8fb18468",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ordinary Least Squares (OLS)\n",
    "\n",
    "First, we are going to fit a plain OLS regression model to act as our baseline model for comparing with other regression methods and see their improvements or weakness when applying regularization or boosting and bootstrapping. By definition, a generic linear regression model is explained by the following:\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_i x_i + \\epsilon_i \\quad \\text{for} \\quad i = 1, \\dots, n$$\n",
    "\n",
    "whereas $y$ is the dependent variable, or variable we are trying to inference or estimate, and $\\beta_j \\quad \\forall j \\in [0, \\inf)$ are estimates or weights of the explanatory/independent variables $x_k \\quad \\forall k \\in [1, \\inf]$, and $\\beta_0$ is a special case, such it is the value of estimated $y$, where all the independent variables equal to 0.\n",
    "\n",
    "Hence, above equation can be generalized into matrix form below:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "where $X$ is the design matrix with leading column of 1s (to represent the intercept term) and columns of independent variables $x_1, \\dots, x_n$, $\\beta$ is the matrix of all estimates from $\\beta_0, \\dots, \\beta_n$, and $\\epsilon$ is the  random error of measurements\n",
    "\n",
    "Then solving for $\\beta$ yields to the following:\n",
    "\n",
    "$$\\beta = (X^{T}X)^{-1}X^{T}Y$$\n",
    "\n",
    "Hence, we could use this above to solve for our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3bf80a2e-b35b-4a99-8267-a8797d817a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of OLS in data\n",
    "\n",
    "# known B = (XTX)^-1XT Y\n",
    "def OLS(X, y):\n",
    "    \"\"\"\n",
    "    Converts the parameters to numpy arrays and perform matrix multiplication to get betas of OLS from\n",
    "    (X^TX)^-1 X^T y\n",
    "    \"\"\"\n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7b89554d-a4a2-49f9-b4ba-0f1016db6862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.51730634e-02,  5.72367308e-02,  6.79200384e-03,  2.84224158e+00,\n",
       "       -4.18639053e+00,  5.50728988e+00, -2.03299765e-03, -8.58683681e-01,\n",
       "        2.03077836e-01, -1.14568253e-02, -2.69615205e-01,  1.39765816e-02,\n",
       "       -3.58173964e-01])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OLS(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2633f280-640d-4424-9400-b6f7df85b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7c30ef85-aacd-411f-b2fa-1915f8d00848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype=object)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c08d90a5-d66f-45d5-ba2e-3fae2d636e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.09502959e-01,  5.28459780e-02,  5.98530796e-02,  2.98410370e+00,\n",
       "       -2.03896996e+01,  3.19138546e+00,  4.11623378e-03, -1.36446270e+00,\n",
       "        3.48832243e-01, -1.56439033e-02, -8.16635448e-01,  7.65523888e-03,\n",
       "       -4.74664528e-01])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5849f01-ab04-4d78-b147-ed652cffc45c",
   "metadata": {},
   "source": [
    "#### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79f120-9c80-4aab-a369-68a57ffb2bf5",
   "metadata": {},
   "source": [
    "#### LASSO Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdd36e-3e12-4ea5-8fe2-dbfeee36e33a",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac7987-bb1f-407a-952b-dfcd0c9ba493",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b931189-4ff9-4364-b9e7-6f80bfd6c0d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Division of Labor\n",
    "Based on the previous discussions, the team has divided the responsibilities as follows:\n",
    "\n",
    "- Tony: Coding\n",
    "- Wanxin: Coding and some textual descriptions\n",
    "- Xuan: Written section of the report\n",
    "\n",
    "However, the team may make adjustments to the division of labor as needed during the project to ensure that all tasks are completed efficiently and effectively. Effective communication and collaboration within the team will be critical to ensure that everyone is working together towards the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b5b37-1f71-4450-a8b7-bcf375b0932f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Vishal, V. (2017, October 27). Boston Housing Dataset. Kaggle. Retrieved March 14, 2023, from https://www.kaggle.com/datasets/altavish/boston-housing-dataset "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ323]",
   "language": "python",
   "name": "conda-env-econ323-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
