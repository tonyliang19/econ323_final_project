{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a028c846-e33d-4839-b554-0ce4c78f4a1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Group Members:** Tony Liang (004), Wanxin luo (003), Xuan Chen (004)\n",
    "\n",
    "**Student Numbers:** 39356993, 33432808, 15734643\n",
    "\n",
    "\n",
    "ECON 323 Quantitative Economic Modelling with Data Science Applications UBC 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fb6f8-bfdd-4817-a220-599c2a2d2d5d",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccee084-e0f4-4a04-a64a-b4acf1a57a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of libraries\n",
    "\n",
    "# essential utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# Models (linear and tree)\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# custom function imports\n",
    "from src.handle_data import get_data, split_data, merge_data\n",
    "from src.modelling import mean_std_cross_val_scores, fit_model, get_metrics\n",
    "from src.plotting import eda_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee480b-3072-4d76-b7aa-c32eb9b9a8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4028d36-64c6-4cef-83e4-b6403436752a",
   "metadata": {},
   "source": [
    "In this project, we aim to **explore the impact of environmental factors on housing prices** using the [Boston Housing dataset](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data.). This dataset contains information on various attributes such as crime rate, average number of rooms, accessibility to highways, and more, which are hypothesized to influence housing prices. The project will involve several parts, including data cleaning, visualization, and model building. Our objective is to conduct exploratory data analysis (EDA) and then build a hedonic regression model with multiple inputs. We will utilize various Python techniques learned in this course to explore the real world data and solve economic questions.\n",
    "\n",
    "By analyzing the data, we aim to answer economic questions related to the housing market and explore the real-world application of Python techniques. It is important to note that the dataset has its limitations as it was collected almost 50 years ago, but it still provides an excellent opportunity for us to apply our Python skills and gain insights of housing market.\n",
    "\n",
    "> Original\n",
    "<!--- Original--->\n",
    "---\n",
    "<!--- Edit from Sherry--->\n",
    "\n",
    "Boston, a thriving metropolitan city known for its rich history and cultural significance, has grappled with the challenge of crime rateand its potential impact on housing prices. Based on past data, the crime rate in Boston is nearly double the national crime rate.In this research, we aim to shed light on the relationship between crime rate and housing prices in Boston, drawing on a hedonic regression model to quantitatively analyze data. Drawing on a comprehensive set of variables, including not only structural characteristics of the housing but also hedonic variables such as crime occurrences in the neighborhood, toursim factor as Charles River and lower status of the population, we seek to provide insights into how crime impacts housing prices in Boston, and to what extent it influences the dynamics of the local real estate market. Our hypothesis aim to show that the **increasing criminal rate in Boston is a significant factor in the determination of housing market prices**.\n",
    "\n",
    "> Sherry edit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f33ad8-1d26-4fee-bef7-da08aec438f9",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e018202-c9b9-4f9c-ba64-ba7f8167765a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston, MA. The following describes the dataset columns:\n",
    "\n",
    "- `CRIM` - per capita crime rate by town\n",
    "- `ZN` - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- `INDUS` - proportion of non-retail business acres per town.\n",
    "- `CHAS` - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- `NOX` - nitric oxides concentration (parts per 10 million)\n",
    "- `RM` - average number of rooms per dwelling\n",
    "- `AGE` - proportion of owner-occupied units built prior to 1940\n",
    "- `DIS` - weighted distances to five Boston employment centres\n",
    "- `RAD` - index of accessibility to radial highways\n",
    "- `TAX` - full-value property-tax rate per $10,000$\n",
    "- `PTRATIO` - pupil-teacher ratio by town\n",
    "- $B - 1000(Bk - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "- `LSTAT` - $%$ lower status of the population\n",
    "- `MEDV` - Median value of owner-occupied homes in $1000$'s\n",
    "\n",
    "The dataset is derived from https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data.\n",
    "\n",
    "> Original\n",
    "<!--- Original --->\n",
    "---\n",
    "<!--- Edit --->\n",
    "Housing is a heterogenous good, which comprises a collection of attributes, and its price is determined by the implicit prices of those attributes. The attributes that determine the price of housing can be broadly classified into three groups: site characteristics, neighborhood characteristics, and environmental characteristics. \n",
    "- The site characteristics include： `ZN`、`RM`、`AGE`、`TAX`、`MEDV`\n",
    "- The neighborhood characteristics include：`CRIM`、`INDUS`、`CHAS`、`DIS`、`RAD` 、`PTRATIO`、`LSTAT`、`B`\n",
    "- The environmental characteristics include：`NOX`\n",
    "\n",
    "> Sherry edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5dc5e-e263-48ce-84e7-250773a1e413",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89eb50-575a-4ad2-81d6-cdb95409b0c0",
   "metadata": {},
   "source": [
    "This report strives to be trustworthy using the following steps: \n",
    "\n",
    "1. [Thorough EDA](#eda)\n",
    "2. [Model fitting](#model-fitting)\n",
    "3. [Model selection](#model-selection)\n",
    "\n",
    "**Note**: this could be subjected to changes later after feedback from the ECON323 Instrutor's team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880d72c-e5ed-4150-af2d-85ec545002f7",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "During the EDA phrase, we will conduct a thorough examination of the Boston Housing dataset. One of the key steps is to generate a correlation matrix, which can help us identify any potential issues related to multicollinearity between the independent variables. In addition, we will use side-by-side box plots to visualize the distributions of the continuous variables and detect any potential outliers or anomalies. Moreover, we will leverage other data visualization techniques, such as scatter plots and histograms, to better understand the relationships between the variables and explore potential trends or patterns in the data. Overall, the goal of EDA is to gain insights into the data and inform our subsequent modelling steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85fda3d-2402-42ac-9ce9-8c907b77f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90    NaN  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset\n",
    "data_path = 'data/boston_housing_data.csv'\n",
    "boston = get_data(data_path=data_path)\n",
    "# check the first rows of the dataset\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7faec-acfd-4c16-9bfe-5b68ac9f9324",
   "metadata": {},
   "source": [
    "Upon observing the first 5 rows of the data, we can identify the categorical and continuous variables. Although most of features are numeric variables, there are still some special variables. For instance, the `CHAS` variable is a dummy variable indicating whether the tract bounds the Charles River or not, and it is encoded as 0 or 1. Additionally, the `RAD` variable is an indexed variable that may be dropped in the future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6bad13-11b7-4fe9-a594-37637022415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize the dataset \n",
    "# boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e457d9-4f2d-445e-b487-076107ec797a",
   "metadata": {},
   "source": [
    "According to the summary table above, we can see the basic statistical information for numerical data, including count, mean, standard deviation, minimum, 25% percentile, median, 75% percentile, and maximum. We can see that the count of each column is different, which means we have missing values here. Therefore, we will check and handle the missing values by using imputation while fitting models.\n",
    "\n",
    "Certainly, there is one column in the dataset that requires special attention, which is the `CHAS` column. `CHAS` is a dummy variable that takes a value of 1 if the property is adjacent to the Charles River, and 0 otherwise. The `CHAS` are 0 for the 25th, 50th, and 75th percentiles, indicating that the distribution of this variable is skewed. To gain a deeper understanding of the data and identify any interesting trends or statistics, it may be beneficial to visualize the dataset through plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d640981-9bf3-46be-9a70-173d707f8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 1: Boxplot of Boston Housing Features', option = \"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a8e73-a764-4109-a069-32b83b232ed3",
   "metadata": {},
   "source": [
    "According to the Figure 1 above, we can observe that several columns in the Boston dataset such as `CRIM`, `ZN`, `RM`, and `B` appear to have many outliers. In the context of machine learning, extreme outliers can be problematic because they can skew statistical measures such as mean and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b122362-7357-42b5-8be0-efdbf9e3f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 2: Histogram of Boston Housing Features', option = \"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2024356-c17b-4c08-927e-77565e91c85d",
   "metadata": {},
   "source": [
    "In addition to identifying outliers, it is also important to understand the distributions of the individual columns in the dataset. The histograms generated earlier suggest that columns `CRIM`, `ZN`, `LSTAT`, `DIS` and `B` have skewed distributions. \n",
    "The distributions of the other columns appear to be normal or bimodal, except for `CHAS`, which is a discrete variable.\n",
    "In contrast, the histogram of `MEDV` (the variable we are trying to predict) appears to have a roughly normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c69bc9-c837-44e4-adfa-ffd4e67c8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 3: Correlation Matrix Heatmap for Boston Dataset', option = \"heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff263da2-25bb-4232-a588-47a5aabaf9e6",
   "metadata": {},
   "source": [
    "Based on the Figure 3 above, it can be observed that there are several variables that exhibit a strong correlation with the target variable, `MEDV`. Specifically, the variable `RM` exhibits a correlation coefficient of $0.7$ with `MEDV`, while `LSTAT` exhibits a correlation coefficient of $0.74$ with `MEDV`. These correlations are relatively high compared to the other variables, and suggest that `RM` and `LSTAT` may be useful predictors for our regression model.\n",
    "\n",
    "However, it is also important to note that the variable `TAX` exhibits a strong correlation with the variable `RAD`, with a correlation coefficient of $0.91$. This suggests the presence of multicollinearity, which can cause issues in regression analysis such as inflated standard errors, reduced statistical power, and unstable coefficients. Therefore, it may be necessary to address multicollinearity in the modeling process, perhaps deleting one of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcd56b4-10bb-4f2b-ba50-c2b04aed4dde",
   "metadata": {},
   "source": [
    "> I honestly think below is redundant "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7af5371-ed1e-417a-af24-68db4073a8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns #RM has the highest correlaton\n",
    "# import matplotlib.pyplot as plt\n",
    "# bos_df_univariate = boston[['RM', \"MEDV\"]]\n",
    "# sns.pairplot(bos_df_univariate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c37b4-ea75-4e34-babe-82d8bfe1b545",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154827a-4a3c-49de-a1a8-72fb9320df95",
   "metadata": {},
   "source": [
    "In the model fitting phase, we will split the Boston Housing dataset into training and testing sets. We will then use the training set to select the relevant variables and build our final multiple linear regression model. The selection process can involve various techniques, such as stepwise regression or regularization, depending on the specific requirements of the project. Once we have the final model, we will use the testing set to evaluate its performance in terms of mean squared error (`MSE`). The goal is to ensure that the model can generalize well to new, unseen data and make accurate predictions. \n",
    "\n",
    "To further explore effects of using different methods of regression, we are going to fit multiple models to using similar metrics accross these to compare best fit of model, i.e. Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for prediction purposes models. Our ideal approach to test this is to fit the following models:\n",
    "1. Ordinary Least Squares (OLS) as baseline \n",
    "2. OLS with L1-norm regularization (Least Absolute Shrinkage and Selection Operator (LASSO Regression) )\n",
    "3. Random Forest Regression\n",
    "\n",
    "> Note: We are only interested in making predictions only, so this part of **Model Fitting** might need to refactor a bit\n",
    ">\n",
    "> Essentially get rid of Adjusted $R^2$ and BIC stuffs, we should only keep RMSE, MSE, MAE (could also be optional, but need to let me know beforehand to edit code correspondingly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ce80a-1ff7-45e6-9083-e5f5a2d850ef",
   "metadata": {},
   "source": [
    "In order to compare different machine learning models, we will use the same scoring metrics(defined below) across all models. We will first perform a raw fit to all models to see how they perform against the test set. This step will give us a baseline performance of each model before applying any preprocessing or hyperparameter optimization.\n",
    "\n",
    "Next, we will apply common preprocessing steps to all models, such as scaling or imputation. After applying the preprocessing steps, we will optimize the hyperparameters for each model using 5-fold cross-validation. This step will help us identify the best combination of hyperparameters for each model.\n",
    "\n",
    "Finally, we will compare the performance of all models again against the test set by using the same scoring metrics. This comparison will help us identify the most suitable model for our prediction problem.\n",
    "\n",
    "> This is just an outline of what to do, It is just like a reminder, and could be better phrased. \n",
    "\n",
    "> Update: already phrased by Wanxin Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25328620-3308-440b-8c05-da254cd7e53f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load and Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50397b-020e-4fec-8abc-a40fd81a3961",
   "metadata": {},
   "source": [
    "To begin, we will split the data into train and test data for further modelling purposes. The training set will be used to train the three models and find the best hyperparameters. The testing set will be used to evaluate each model's performance on previously unseen data, which helps us to assess models' ability to generalize beyond the training set. Update：浅改了一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68b376a9-b0a7-4027-803c-c295306f42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same random_state across all parts to ensure reproducibility\n",
    "random_state = 20230325\n",
    "\n",
    "# Splits the data into X and y train and test portions\n",
    "X_train, X_test, y_train, y_test = split_data(data_path, proportion = 0.75, target = \"MEDV\", random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3b0a7-e7a0-47a9-a320-39e38143a972",
   "metadata": {},
   "source": [
    "Moreover, we are going to use common scoring metrics (as mentioned earlier) across all models, so that we could compare their effectiveness in prediction:\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Percentange Error (MAPE)\n",
    "- Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bed975f-0833-4bdf-997a-ba3129ccc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define common scoring across models\n",
    "scoring = {\n",
    "    \"neg_rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mape\": \"neg_mean_absolute_percentage_error\", \n",
    "    \"neg_mse\": \"neg_mean_squared_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabc9a4-fd6b-441e-a10a-cefb367f1edf",
   "metadata": {},
   "source": [
    "We could fit all the previous models mentioned at once and perform cross validation, in a more rapid way to compare and check which might perform better at **predicting house prices**.\n",
    "Then we could fit the methods individually in sections below raw, then with preprocessing, and check if getting similar scores like these CVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f1c4785-97c1-4ea8-a918-c93618df0c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_309c7\">\n",
       "  <caption>Means CVs and +- std for Regression Methods</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_309c7_level0_col0\" class=\"col_heading level0 col0\" >OLS</th>\n",
       "      <th id=\"T_309c7_level0_col1\" class=\"col_heading level0 col1\" >LASSO</th>\n",
       "      <th id=\"T_309c7_level0_col2\" class=\"col_heading level0 col2\" >Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row0\" class=\"row_heading level0 row0\" >fit_time</th>\n",
       "      <td id=\"T_309c7_row0_col0\" class=\"data row0 col0\" >0.003 (+/- 0.006)</td>\n",
       "      <td id=\"T_309c7_row0_col1\" class=\"data row0 col1\" >0.000 (+/- 0.000)</td>\n",
       "      <td id=\"T_309c7_row0_col2\" class=\"data row0 col2\" >0.179 (+/- 0.011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row1\" class=\"row_heading level0 row1\" >score_time</th>\n",
       "      <td id=\"T_309c7_row1_col0\" class=\"data row1 col0\" >0.000 (+/- 0.000)</td>\n",
       "      <td id=\"T_309c7_row1_col1\" class=\"data row1 col1\" >0.003 (+/- 0.006)</td>\n",
       "      <td id=\"T_309c7_row1_col2\" class=\"data row1 col2\" >0.009 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row2\" class=\"row_heading level0 row2\" >test_rmse</th>\n",
       "      <td id=\"T_309c7_row2_col0\" class=\"data row2 col0\" >4.520 (+/- 0.788)</td>\n",
       "      <td id=\"T_309c7_row2_col1\" class=\"data row2 col1\" >5.003 (+/- 0.585)</td>\n",
       "      <td id=\"T_309c7_row2_col2\" class=\"data row2 col2\" >3.387 (+/- 0.442)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row3\" class=\"row_heading level0 row3\" >train_rmse</th>\n",
       "      <td id=\"T_309c7_row3_col0\" class=\"data row3 col0\" >4.278 (+/- 0.197)</td>\n",
       "      <td id=\"T_309c7_row3_col1\" class=\"data row3 col1\" >4.858 (+/- 0.167)</td>\n",
       "      <td id=\"T_309c7_row3_col2\" class=\"data row3 col2\" >1.332 (+/- 0.130)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row4\" class=\"row_heading level0 row4\" >test_mape</th>\n",
       "      <td id=\"T_309c7_row4_col0\" class=\"data row4 col0\" >0.163 (+/- 0.020)</td>\n",
       "      <td id=\"T_309c7_row4_col1\" class=\"data row4 col1\" >0.177 (+/- 0.023)</td>\n",
       "      <td id=\"T_309c7_row4_col2\" class=\"data row4 col2\" >0.125 (+/- 0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row5\" class=\"row_heading level0 row5\" >train_mape</th>\n",
       "      <td id=\"T_309c7_row5_col0\" class=\"data row5 col0\" >0.152 (+/- 0.006)</td>\n",
       "      <td id=\"T_309c7_row5_col1\" class=\"data row5 col1\" >0.168 (+/- 0.004)</td>\n",
       "      <td id=\"T_309c7_row5_col2\" class=\"data row5 col2\" >0.044 (+/- 0.001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row6\" class=\"row_heading level0 row6\" >test_mse</th>\n",
       "      <td id=\"T_309c7_row6_col0\" class=\"data row6 col0\" >20.925 (+/- 7.115)</td>\n",
       "      <td id=\"T_309c7_row6_col1\" class=\"data row6 col1\" >25.308 (+/- 5.804)</td>\n",
       "      <td id=\"T_309c7_row6_col2\" class=\"data row6 col2\" >11.625 (+/- 2.868)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_309c7_level0_row7\" class=\"row_heading level0 row7\" >train_mse</th>\n",
       "      <td id=\"T_309c7_row7_col0\" class=\"data row7 col0\" >18.329 (+/- 1.684)</td>\n",
       "      <td id=\"T_309c7_row7_col1\" class=\"data row7 col1\" >23.627 (+/- 1.630)</td>\n",
       "      <td id=\"T_309c7_row7_col2\" class=\"data row7 col2\" >1.789 (+/- 0.339)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x20849be7dd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fit all models at once to decide which one to use\n",
    "# # User-defined scoring to be hanlde in-terms of calculating scores for model\n",
    "# # Note scikit learn implemented these scorings in negative scale to minimize loss\n",
    "# # which we will scale back to positive behind the scenes\n",
    "\n",
    "def fit_all(scoring=scoring, **kwargs):\n",
    "    methods = [\"OLS\", \"LASSO\", \"Random Forest\"]\n",
    "    models = [mod for mod in [LinearRegression(), Lasso(), RandomForestRegressor()]]\n",
    "    # allocate space to store model fitted outputss\n",
    "    result_dict = {}\n",
    "    # fit all of these models at once\n",
    "    for method, model in zip(methods, models):\n",
    "        result_dict[method] = mean_std_cross_val_scores(model, \n",
    "                                                       X_train, \n",
    "                                                       y_train, \n",
    "                                                       return_train_score=True,\n",
    "                                                       scoring=scoring\n",
    "                                                     )\n",
    "    \n",
    "    result = pd.DataFrame(result_dict)\n",
    "    # rename the index to positive scale for easier comparison, replace the neg by empty\n",
    "    return result.rename(lambda x: x.replace(\"_neg\", \"\"))\n",
    "all_cv = fit_all(X_train=X_train, y_train=y_train)\n",
    "all_cv.style.set_caption(\"Means CVs and +- std for Regression Methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc3731-cefb-44a1-be74-23d22c983272",
   "metadata": {},
   "source": [
    "From above table, we could see fit time, score time, the scorings we set earlier **(MAPE, RMSE, MSE)**. And, each entry is the mean cross-validation (cv) scores $\\pm$ the standard deviation. \n",
    "\n",
    "Based on the test scores, the random forest (RF) model performs the best, this is expected since trees could capture non-linear relationships (if existed) between variables. Also, it trains multiple decision trees simultaneously and reduces variance through bagging or bootstrap over random subsets of variables and observations.  \n",
    "\n",
    "For linear models, we observed that plain Ordinary Least Squares (OLS) performs better than LASSO. This could be due to the fact that we only used the default value for the regularization parameter $\\alpha$ in LASSO, which might not have selected enough variables. Furthermore, OLS by definition minimizes the prediction error, which is why it outperforms LASSO with default parameters.\n",
    "\n",
    "For more information, we could also look at training scores, the tree model has the lowest **train** score, which is also expected, since trees are tended to overfit if we set the depth to infinity or if not constrained. However, it takes longer fitting time due to its nature of bagging an ensemble of multiple regression trees. \n",
    "\n",
    "> I wrote this part by plain memory, some stuffs might not be accurate or true. Just like a general idea of what interpreation could be provide here @Tony\n",
    "\n",
    "> \"And it reduces variance (?) by bagging or bootstrap over random subsets of variables and observations.\" This is right, I the rephrased the entire paragraph a little bit. @ Wanxin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2371b-5c0d-473b-bcae-78f23abde29d",
   "metadata": {},
   "source": [
    "Based on the preliminary test scores, we might have some sense of which models might work better for predicting housing prices in Boston. We hypothesize that RF might the best model, and OLS would serve as our baseline model to compare. Moving forward, we will fit all models on both raw and preprocessed data with optimized hyperparameters to determine the best model for predicting housing prices in Boston. Of course, OLS would serve as our baseline model to compare. \n",
    "\n",
    "> 改了一点by Wanxin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f773a-254b-4d48-bfce-767007ffe81c",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares (OLS)\n",
    "In the first stage of modeling, we will apply the traditional hedonic ordinary least squares regression to determine the coefficients between housing characteristics and housing prices. With the OLS approach, we can estimate the impact of each attribute on the market value of a heterogeneous good, and then we estimate prices in the form of willingness to pay for one unit of change of that characteristic.\n",
    "\n",
    "Now, we are going to fit a plain OLS regression model to act as our baseline model for comparing with other regression methods and see their improvements or weakness when applying regularization or boosting and bootstrapping. We propose a mixed model with different exponents for the explanatory variables as following form:\n",
    "\n",
    "$$P = \\beta_0 + \\beta_1 \\text{CRIM} + \\beta_2 \\text{ZN} +\\beta_3 \\text{INDUS}+\\beta_4 \\text{CHAS}+\\beta_5 \\text{NOX}+\\beta_6 \\text{RM}+\\beta_7 \\text{AGE}+\\beta_8 \\text{DIS}+\\beta_9 \\text{RAD}+\\beta_{10} \\text{TAX}+\\beta_{11} \\text{PTRATIO}+\\beta_{12} \\text{B}+\\beta_{13} \\text{LSTAT}Price of housing unit in Boston$$\n",
    "\n",
    "whereas $P$ is the dependent variable `Price of housing unit in Boston`, and other $\\beta_i$ are beta estimates of independent variables $x_i \\quad \\forall i \\in [0, 13]$, and $\\beta_0$ is a special case, such it is the value of estimated $y$, where all the independent variables equal to 0.\n",
    "\n",
    "Hence, above equation can be generalized into matrix form below:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "where $X$ is the design matrix with leading column of 1s (to represent the intercept term) and columns of independent variables $x_1, \\dots, x_{14}$, $\\beta$ is the matrix of all estimates from $\\beta_0, \\dots, \\beta_{14}$, and $\\epsilon$ is the  random error of measurements\n",
    "\n",
    "Then solving for $\\beta$ yields to the following:\n",
    "\n",
    "$$\\beta = (X^{T}X)^{-1}X^{T}Y$$\n",
    "\n",
    "Hence, we could use this above to solve for our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b009eea4-8779-4b68-8dc8-b10598a3b03d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m raw_lr_scores \u001b[38;5;241m=\u001b[39m get_metrics(predicted\u001b[38;5;241m=\u001b[39mraw_lr\u001b[38;5;241m.\u001b[39mpredict(X_test), actual\u001b[38;5;241m=\u001b[39my_test, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOLS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# preprocess train and use that to train model and predict on test\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m lr_tuned \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOLS\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# merge results and show it\u001b[39;00m\n\u001b[0;32m      7\u001b[0m lr_mods \u001b[38;5;241m=\u001b[39m merge_data(raw_lr, lr_tuned)\n",
      "Cell \u001b[1;32mIn[12], line 23\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(X_train, y_train, X_test, y_test, name, preprocess, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m make_pipeline(preprocessor, mod)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# fit the data (applying transformations) and predict on test\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pipe\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#predicted = pipe.predict(X_test)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#result = get_metrics(actual=y_test, predicted=predicted, name=name, preprocess=True)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#return result#.style.set_caption(f\"Test Scores on {name} Regression with preprocessing transformations\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# fit to train data and predict on test\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m \u001b[43mfit_transform_one_cached\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcloned_transformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage_clsname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPipeline\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:724\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    723\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_transformers()\n\u001b[1;32m--> 724\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_column_callables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_remainder(X)\n\u001b[0;32m    727\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform(X, y, _fit_transform_one)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\compose\\_column_transformer.py:426\u001b[0m, in \u001b[0;36mColumnTransformer._validate_column_callables\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    424\u001b[0m         columns \u001b[38;5;241m=\u001b[39m columns(X)\n\u001b[0;32m    425\u001b[0m     all_columns\u001b[38;5;241m.\u001b[39mappend(columns)\n\u001b[1;32m--> 426\u001b[0m     transformer_to_input_indices[name] \u001b[38;5;241m=\u001b[39m \u001b[43m_get_column_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_columns \u001b[38;5;241m=\u001b[39m all_columns\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transformer_to_input_indices \u001b[38;5;241m=\u001b[39m transformer_to_input_indices\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\econ323\\Lib\\site-packages\\sklearn\\utils\\__init__.py:460\u001b[0m, in \u001b[0;36m_get_column_indices\u001b[1;34m(X, key)\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column_indices\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid specification of the columns. Only a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscalar, list or slice of all integers or all \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings, or boolean mask is allowed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    464\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed"
     ]
    }
   ],
   "source": [
    "# call custom helper to fit OLS model and predict on test data, based on selected metrics\n",
    "raw_lr = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"OLS\")\n",
    "raw_lr_scores = get_metrics(predicted=raw_lr.predict(X_test), actual=y_test, name=\"OLS\")\n",
    "# preprocess train and use that to train model and predict on test\n",
    "lr_tuned = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"OLS\", preprocess=True)\n",
    "lr_tuned_scores = get_metrics(predicted=lr_tuned.predict(X_test), actual=y_test, name=\"OLS Predicted\")\n",
    "# merge results and show it\n",
    "lr_mods = merge_data(raw_lr_scores, lr_tuned_scores)\n",
    "lr_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be1b02-80aa-4479-a916-bbdc3def1104",
   "metadata": {},
   "source": [
    ">  * RMSE is the square root of Mean Squared error and measure the standard deviation of residuals,which tells us the typical distance between the predicted value made by the regression model and the actual value. In the \"OLS\" model, the RMSE is 4.954, while in the \"OLS + preprocessed\" model, the RMSE is slightly lower at 4.922. This suggests that the \"OLS + preprocessed\" model may have slightly better predictive performance compared to the \"OLS\" model.\n",
    "> * MSE is average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals. In the \"OLS\" model, the MSE is 24.539, while in the \"OLS + preprocessed\" model, the MSE is slightly lower at 24.224. This further supports the observation that the \"OLS + preprocessed\" model may have slightly better predictive performance compared to the \"OLS\" model.\n",
    "> * MAPE is represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals. In the \"OLS\" model, the MAPE is 0.161, while in the \"OLS + preprocessed\" model, the MAPE is lower at 0.152. This suggests that the \"OLS + preprocessed\" model may have better predictive performance in terms of percentage errors compared to the \"OLS\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782131d-252e-436b-85b1-004a6e48c1ec",
   "metadata": {},
   "source": [
    "#### Multiple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e6ef5-4da4-44a7-8363-1f2dab7a50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statsmodels.api as sm\n",
    "\n",
    "# X = X_train[['CRIM', 'ZN', 'INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']]\n",
    "# y = y_train\n",
    "\n",
    "# #add constant to predictor variables\n",
    "# X = sm.add_constant(X)\n",
    "\n",
    "# #fit linear regression model\n",
    "# model = sm.OLS(y, X).fit()\n",
    "\n",
    "# #store parameter estimates\n",
    "# beta_0_multi = model.params['const']\n",
    "# beta_1_multi = model.params['CRIM']\n",
    "# beta_2_multi = model.params['ZN']\n",
    "# beta_3_multi = model.params['INDUS']\n",
    "# beta_4_multi = model.params['CHAS']\n",
    "# beta_5_multi = model.params['NOX']\n",
    "# beta_6_multi = model.params['RM']\n",
    "# beta_7_multi = model.params['AGE']\n",
    "# beta_8_multi = model.params['DIS']\n",
    "# beta_9_multi = model.params['RAD']\n",
    "# beta_10_multi = model.params['TAX']\n",
    "# beta_11_multi = model.params['PTRATIO']\n",
    "# beta_12_multi = model.params['B']\n",
    "# beta_13_multi = model.params['LSTAT']\n",
    "\n",
    "# #view fitted model\n",
    "# print(f\"Price of housing price= {beta_0_multi}+ {beta_1_multi:.4f} CRIM + {beta_2_multi:.4f}ZN+{beta_3_multi:.4f} INDUS+{beta_4_multi:.4f} CHAS+{beta_5_multi:.4f} NOX + {beta_6_multi:.4f} RM + {beta_7_multi:.4f} AGE+{beta_8_multi:.4f} DIS+{beta_9_multi:.4f} RAD + {beta_10_multi:.4f} TAX +{beta_11_multi:.4f} PTRATIO+{beta_12_multi:.4f} B+{beta_13_multi:.4f} LSTAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babafd2-1136-4aa4-bfe7-522cf75eb3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e7d8f-f02b-4ea6-8414-2fec6f56971c",
   "metadata": {
    "tags": []
   },
   "source": [
    "> Add interpretation here\n",
    "\n",
    "> * Interpretation of intercept:The intercept gives us the average housing price in boston,which is 38.8110 in this sample.\n",
    "> * Interpretation of each regressors: For CRIM: the coefficient for CRIM is -0.1095, which means that for every one-unit increase in CRIM, the estimated change in MEDV is a decrease of 0.1095 units, holding all other variables constant.\n",
    "> * Using the F statistics to determine the significantly difference: Our F-statistic result is 64.30, with a very low p-value of 2.45e-76, which indicating that  we have strong evidence against the null hypothesis that all regression coefficients are zero and then the regression model is statistically significant.\n",
    "> * In this fitted model, above table shows the estimated coefficients for each explantory variables. The R-squared means that approximately 74.8% of total valuation is explained by the regression model and we obtained a value for the adjusted determination coefficient of 0.736, which indicates that we have a good fit. Additionally, most of the variables are significant, and they have the expected sign.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e71abb-6fd1-44fa-872b-524f828dcf71",
   "metadata": {},
   "source": [
    "* For this part,we want to use Variance Inflation Factor to check the Multicollinearity,if there are any high correlations among the predictor variables, then we proceeded to eliminate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf30365-0ddf-4817-b33a-e23fb8e184cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # the independent variables set\n",
    "# X2 = X_train[['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
    "  \n",
    "# # VIF dataframe\n",
    "# vif2_data = pd.DataFrame()\n",
    "# vif2_data[\"Feature\"] = X2.columns\n",
    "  \n",
    "# # calculating VIF for each feature\n",
    "# vif2_data[\"VIF\"] = [variance_inflation_factor(X2.values, i)\n",
    "#                           for i in range(len(X2.columns))]\n",
    "  \n",
    "# print(vif2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b868ef-a014-4e20-bd81-594e91a08e70",
   "metadata": {},
   "source": [
    "* !!! Yes,we have super high VIF at here, we need to drop until the each feature's VIF is less than 10.\n",
    "* firstly, we drop the TAX,PTRATIO,NOX,RM,B,Age because those are extremely big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b6f60e-8199-453f-b17d-d6ff18622455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the independent variables set\n",
    "# X2 =  X_train[['CRIM', 'ZN', 'INDUS', 'CHAS', 'DIS', 'RAD', 'LSTAT']]\n",
    "  \n",
    "# # VIF dataframe\n",
    "# vif2_data = pd.DataFrame()\n",
    "# vif2_data[\"Feature\"] = X2.columns\n",
    "  \n",
    "# # calculating VIF for each feature\n",
    "# vif2_data[\"VIF\"] = [variance_inflation_factor(X2.values, i)\n",
    "#                           for i in range(len(X2.columns))]\n",
    "\n",
    "# print(vif2_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c01241b-ad2a-4437-9dc9-3bd199180784",
   "metadata": {},
   "source": [
    "* Now,all of the variable is less than 10! We deducts variables who has too much correlation between the variable Xj and the other explanatory variables. \n",
    "* We construct a new model with no problem of multicollinearity according to the VIF analysis since all the variables in this model had a VIF under 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12baa7dc-38cf-4d46-b3ba-198d42022e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X_train[['CRIM', 'ZN', 'INDUS','CHAS','DIS','RAD','LSTAT']]\n",
    "# y = y_train\n",
    "\n",
    "# #add constant to predictor variables\n",
    "# X = sm.add_constant(X)\n",
    "\n",
    "# #fit linear regression model\n",
    "# model2 = sm.OLS(y, X).fit()\n",
    "\n",
    "# #store parameter estimates\n",
    "# beta_0_multi = model.params['const']\n",
    "# beta_1_multi = model.params['CRIM']\n",
    "# beta_2_multi = model.params['ZN']\n",
    "# beta_3_multi = model.params['INDUS']\n",
    "# beta_4_multi = model.params['CHAS']\n",
    "# beta_5_multi = model.params['DIS']\n",
    "# beta_6_multi = model.params['RAD']\n",
    "# beta_7_multi = model.params['LSTAT']\n",
    "\n",
    "\n",
    "# #view fitted model\n",
    "# print(f\"Price of housing price= {beta_0_multi}+ {beta_1_multi:.4f} CRIM + {beta_2_multi:.4f}ZN+{beta_3_multi:.4f} INDUS+{beta_4_multi:.4f} CHAS+{beta_5_multi:.4f} DIS + {beta_6_multi:.4f} RAD + {beta_7_multi:.4f} LSTAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317d1a1-57ad-4332-a8ae-d8f8165285fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = LinearRegression()\n",
    "# lm.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lm.predict(X_train)\n",
    "# y_pred\n",
    "\n",
    "# plt.figure(figsize = (10,5))\n",
    "# sns.regplot(x=y_train,y=y_pred)\n",
    "# plt.title('Linear regression with all features', fontsize = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b69eed-a79b-4bae-b71c-51f24a01986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5090bb90-d1fa-4f3b-8b31-b05dd97bef52",
   "metadata": {},
   "source": [
    "* After the Homoscedasticity and nonautocorrelation were verified, while the assumption of normality was not met. To solve the problem of this distributional assumption, we do the following part to let we can obtain more accurate inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79f120-9c80-4aab-a369-68a57ffb2bf5",
   "metadata": {},
   "source": [
    "#### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ae49e-b4ac-4273-b08a-1f7135bdaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call custom helper to fit LASSO model and predict on test data, based on selected metrics\n",
    "raw_lasso = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"LASSO\")\n",
    "# preprocess train and use that to train model and predict on test\n",
    "lasso_tuned = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"LASSO\", preprocess=True)\n",
    "# merge results and show it\n",
    "lasso_mods = merge_data(raw_lasso, lasso_tuned)\n",
    "lasso_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc086-adbc-4b22-95b5-2267fa88f6b1",
   "metadata": {},
   "source": [
    "> Add interpretaion here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac7987-bb1f-407a-952b-dfcd0c9ba493",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d2f5b-d6e5-4672-84b8-a1f2a475d76b",
   "metadata": {},
   "source": [
    "Other than using linear models, we are so using tree-like models, particularly ensembles of multiple decision regression trees which is a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6404885-d253-4f75-88c0-f98f63d8b9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call custom helper to fit Random Forest model and predict on test data, based on selected metrics\n",
    "raw_rf = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"RF\",\n",
    "                   random_state=random_state)\n",
    "# preprocess train and use that to train model and predict on test\n",
    "rf_tuned = fit_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"RF\", \n",
    "                     preprocess=True, random_state=random_state)\n",
    "# merge results and show it\n",
    "rf_mods = merge_data(raw_rf, rf_tuned)\n",
    "rf_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a69bdb-3ba6-4397-93b5-ce44dd48d98b",
   "metadata": {},
   "source": [
    "> Add interpretaion here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78a14a-cf31-4e62-a08c-86758978d12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all results\n",
    "all_mods = merge_data(lr_mods, lasso_mods, rf_mods)\n",
    "# get the mean test cvs scores earlier to compare with the ones with got right now \n",
    "cvs = all_cv.loc[[\"test_rmse\", \"test_mse\", \"test_mape\"]].rename(lambda x: x.replace(\"test_\", \"\").upper()).add_suffix(\"_CV\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ee993-db1d-44ef-a099-197db9254ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data(cvs, all_mods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be02f1e-c803-48b8-8aad-3691f903c7ee",
   "metadata": {},
   "source": [
    "> Add interpretaion here\n",
    ">\n",
    "> Note: I have not got to optimal hyperparameters of LASSO nor Random Forest yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc51ca2-1359-4c5b-b975-a1d2e0678549",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ffc516-0b05-438b-9349-5ea6f2f8dfcb",
   "metadata": {},
   "source": [
    "Based on our initial analysis, the random forest model doesn't demonstrate any significant improvement compared to other models in predicting the housing prices in Boston. Moreover, this model is relatively complex and difficult to interpret, which limits its practical application. Therefore, we decide to use the Lasso regression method to select relevant variables and then fit those selected variables into an OLS model to evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72abd880-5199-4792-890d-e0acde561c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "preprocessor = preprocess_data(X_train)\n",
    "\n",
    "best_score = 0\n",
    "\n",
    "param_grid = {\"alpha\": 10.0 ** np.arange(-3, 6, 1)}\n",
    "\n",
    "results_dict = {\"alpha\": [], \"mean_cv_score\": []}\n",
    "\n",
    "for a in param_grid[\n",
    "    \"alpha\"\n",
    "]:  \n",
    "    pipe = make_pipeline(preprocessor, Lasso(alpha=a))\n",
    "    scores = cross_val_score(pipe, X_train, y_train)  # perform cross-validation\n",
    "    mean_score = np.mean(scores)  # compute mean cross-validation accuracy\n",
    "    if (\n",
    "        mean_score > best_score\n",
    "    ):  # if we got a better score, store the score and parameters\n",
    "        best_score = mean_score\n",
    "        best_params = {\"alpha\": a}\n",
    "    results_dict[\"alpha\"].append(a)\n",
    "    results_dict[\"mean_cv_score\"].append(mean_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110f9c4f-b481-487d-bdaf-3852dae59162",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33608e-b190-4c59-8660-f67808c1f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a62b4-38f9-4c72-94b2-50922b2f89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing steps and Lasso regression\n",
    "pipe_lasso = make_pipeline(preprocessor, Lasso(alpha=0.01))\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Extract the coefficients of the Lasso model\n",
    "coeffs = pipe_lasso.named_steps[\"lasso\"].coef_\n",
    "\n",
    "# Create a DataFrame to display the coefficients\n",
    "pd.DataFrame(coeffs, \n",
    "             index=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX',\n",
    "                    'PTRATIO', 'B', 'LSTAT'], \n",
    "             columns=[\"Coefficients\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f84b8-fb65-485c-8259-459551003279",
   "metadata": {},
   "source": [
    "need to check the performance in the test set, but it's redundant for me to do here, maybe adjust the code in the python scripts in the folder src is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a625f2-fd1f-4eb9-99d2-4837adfb8818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "# modified by Tony Liang\n",
    "def show_feature_importances(mod, X=X_train, plot=False):\n",
    "    # get the importance of variables \n",
    "    importances = mod.feature_importances_\n",
    "    # get stan dard deviation of these variables across individual trees\n",
    "    std = np.std([tree.feature_importances_ for tree in mod.estimators_], axis=0)\n",
    "    # get names of our original variables\n",
    "    feature_names = [col for col in X_train.columns]\n",
    "    # wrap to pandas series\n",
    "    forest_importances = pd.Series(importances, index=feature_names)\n",
    "    \n",
    "    if plot is True:\n",
    "        # plot the feature importances\n",
    "        fig, ax = plt.subplots()\n",
    "        # add std line as xerr\n",
    "        forest_importances.plot.barh(xerr=std, ax=ax)\n",
    "        ax.set_title(\"Variable importances in predicting house price in Boston\")\n",
    "        ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "        fig.tight_layout()\n",
    "    else:\n",
    "        out = forest_importances.to_frame(name=\"Importance\").sort_values(by=\"Importance\", ascending=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36da3fe-d739-40b3-bf52-c8944cc9e7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show plot view of importances\n",
    "# show_feature_importances(mod=optimal_rf, plot=True)\n",
    "# # show dataframe view of importances\n",
    "# importance_df = show_feature_importances(mod=optimal_rf)\n",
    "# importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b931189-4ff9-4364-b9e7-6f80bfd6c0d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Division of Labor\n",
    "Based on the previous discussions, the team has divided the responsibilities as follows:\n",
    "\n",
    "- Tony: Coding\n",
    "- Wanxin: Coding and some textual descriptions\n",
    "- Xuan: Written section of the report\n",
    "\n",
    "However, the team may make adjustments to the division of labor as needed during the project to ensure that all tasks are completed efficiently and effectively. Effective communication and collaboration within the team will be critical to ensure that everyone is working together towards the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b5b37-1f71-4450-a8b7-bcf375b0932f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Vishal, V. (2017, October 27). Boston Housing Dataset. Kaggle. Retrieved March 14, 2023, from https://www.kaggle.com/datasets/altavish/boston-housing-dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e77a0f-fbe7-4f6d-a573-e34b7d87798f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unused Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d274a-bc74-4225-b4f1-0667af82607a",
   "metadata": {},
   "source": [
    "<!-- According to the summary table above, we can see the basic statistical information for numerical data, including count, mean, standard deviation, minimum, 25% percentile, median, 75% percentile, and maximum. \n",
    "\n",
    "Certainly, there is one column in the dataset that requires special attention, which is the CHAS column. CHAS is a dummy variable that takes a value of 1 if the property is adjacent to the Charles River, and 0 otherwise. The CHAS are 0 for the 25th, 50th, and 75th percentiles, indicating that the distribution of this variable is skewed. To gain a deeper understanding of the data and identify any interesting trends or statistics, it may be beneficial to visualize the dataset through plotting.### Model Fitting\n",
    "\n",
    "In the model fitting phase, we will split the Boston Housing dataset into training and testing sets. We will then use the training set to select the relevant variables and build our final multiple linear regression model. The selection process can involve various techniques, such as stepwise regression or regularization, depending on the specific requirements of the project. Once we have the final model, we will use the testing set to evaluate its performance in terms of mean squared error (`MSE`). The goal is to ensure that the model can generalize well to new, unseen data and make accurate predictions. \n",
    "\n",
    "To further explore effects of using different methods of regression, we are going to fit multiple models to using similar metrics accross these to compare best fit of model, i.e. Bayesian Information Criterion (BIC) and Adjusted $R^2$ for inference (how well our model explains the effects of the explanatory variables is of the variable of interes); Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for prediction purposes models. Our ideal approach to test this is to fit the following models:\n",
    "1. Ordinary Least Squares (OLS) as baseline \n",
    "2. OLS with L2-norm regularization (Ridge Regression)\n",
    "3. OLS with L1-norm regularization (Least Absolute Shrinkage and Selection Operator (LASSO Regression) )\n",
    "4. Decision Tree Regression\n",
    "5. Random Forest Regression -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6e7e1-2971-4a92-bae3-a59e4ca48a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercept and beta estimates\n",
    "# scikit-learn imported function\n",
    "# model = LinearRegression().fit(X_train, y_train)\n",
    "# self-defined function\n",
    "# intercept, estimates = OLS(X_train, y_train)\n",
    "# Use this option to NOT show scientific notation (default shows scientific notation)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# # print the estimates from self-defined func\n",
    "# print(f\"From self defined OLS, the intercept is {round(intercept,3)}, \\nAnd beta estimates are: \\n{np.ndarray.round(estimates, decimals=3)}\")\n",
    "\n",
    "# # prints the estimates from scikit-learn to inspect and compare\n",
    "# print(f\"\\nFrom scikit-learn function, the intercept is {round(model.intercept_, 3)}, \\nAnd beta estimates are: \")\n",
    "# print(np.ndarray.round(model.coef_[1:], decimals=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ323]",
   "language": "python",
   "name": "conda-env-econ323-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
