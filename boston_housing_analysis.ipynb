{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a028c846-e33d-4839-b554-0ce4c78f4a1d",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Group Members:** Tony Liang (004), Wanxin luo (003), Xuan Chen (004)\n",
    "\n",
    "**Student Numbers:** 39356993, 33432808, 15734643\n",
    "\n",
    "\n",
    "ECON 323 Quantitative Economic Modelling with Data Science Applications UBC 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67fb6f8-bfdd-4817-a220-599c2a2d2d5d",
   "metadata": {},
   "source": [
    "# Boston Housing Price Prediction Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ccee084-e0f4-4a04-a64a-b4acf1a57a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of libraries\n",
    "\n",
    "# essential utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# pipeline for model\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# Models (linear and tree)\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# custom function imports\n",
    "from src.handle_data import get_data, split_data, preprocess_data\n",
    "from src.modelling import mean_std_cross_val_scores, fit_raw_model\n",
    "from src.plotting import eda_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbee480b-3072-4d76-b7aa-c32eb9b9a8f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4028d36-64c6-4cef-83e4-b6403436752a",
   "metadata": {},
   "source": [
    "In this project, we aim to **explore the impact of environmental factors on housing prices** using the [Boston Housing dataset](https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data.). This dataset contains information on various attributes such as crime rate, average number of rooms, accessibility to highways, and more, which are hypothesized to influence housing prices. The project will involve several parts, including data cleaning, visualization, and model building. Our objective is to conduct exploratory data analysis (EDA) and then build a hedonic regression model with multiple inputs. We will utilize various Python techniques learned in this course to explore the real world data and solve economic questions.\n",
    "\n",
    "By analyzing the data, we aim to answer economic questions related to the housing market and explore the real-world application of Python techniques. It is important to note that the dataset has its limitations as it was collected almost 50 years ago, but it still provides an excellent opportunity for us to apply our Python skills and gain insights of housing market.\n",
    "\n",
    "> Original\n",
    "<!--- Original--->\n",
    "---\n",
    "<!--- Edit from Sherry--->\n",
    "\n",
    "Boston, a thriving metropolitan city known for its rich history and cultural significance, has grappled with the challenge of crime rateand its potential impact on housing prices. Based on past data, the crime rate in Boston is nearly double the national crime rate.In this research, we aim to shed light on the relationship between crime rate and housing prices in Boston, drawing on a hedonic regression model to quantitatively analyze data. Drawing on a comprehensive set of variables, including not only structural characteristics of the housing but also hedonic variables such as crime occurrences in the neighborhood, toursim factor as Charles River and lower status of the population, we seek to provide insights into how crime impacts housing prices in Boston, and to what extent it influences the dynamics of the local real estate market. Our hypothesis aim to show that the **increasing criminal rate in Boston is a significant factor in the determination of housing market prices**.\n",
    "\n",
    "> Sherry edit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f33ad8-1d26-4fee-bef7-da08aec438f9",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e018202-c9b9-4f9c-ba64-ba7f8167765a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston, MA. The following describes the dataset columns:\n",
    "\n",
    "- `CRIM` - per capita crime rate by town\n",
    "- `ZN` - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "- `INDUS` - proportion of non-retail business acres per town.\n",
    "- `CHAS` - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "- `NOX` - nitric oxides concentration (parts per 10 million)\n",
    "- `RM` - average number of rooms per dwelling\n",
    "- `AGE` - proportion of owner-occupied units built prior to 1940\n",
    "- `DIS` - weighted distances to five Boston employment centres\n",
    "- `RAD` - index of accessibility to radial highways\n",
    "- `TAX` - full-value property-tax rate per $10,000$\n",
    "- `PTRATIO` - pupil-teacher ratio by town\n",
    "- $B - 1000(Bk - 0.63)^2$ where $B_k$ is the proportion of blacks by town\n",
    "- `LSTAT` - $%$ lower status of the population\n",
    "- `MEDV` - Median value of owner-occupied homes in $1000$'s\n",
    "\n",
    "The dataset is derived from https://www.kaggle.com/datasets/fedesoriano/the-boston-houseprice-data.\n",
    "\n",
    "> Original\n",
    "<!--- Original --->\n",
    "---\n",
    "<!--- Edit --->\n",
    "Housing is a heterogenous good, which comprises a collection of attributes, and its price is determined by the implicit prices of those attributes. The attributes that determine the price of housing can be broadly classified into three groups: site characteristics, neighborhood characteristics, and environmental characteristics. \n",
    "- The site characteristics include： `ZN`、`RM`、`AGE`、`TAX`、`MEDV`\n",
    "- The neighborhood characteristics include：`CRIM`、`INDUS`、`CHAS`、`DIS`、`RAD` 、`PTRATIO`、`LSTAT`、`B`\n",
    "- The environmental characteristics include：`NOX`\n",
    "\n",
    "> Sherry edit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5dc5e-e263-48ce-84e7-250773a1e413",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af89eb50-575a-4ad2-81d6-cdb95409b0c0",
   "metadata": {},
   "source": [
    "This report strives to be trustworthy using the following steps: \n",
    "\n",
    "1. [Data cleaning](#data-cleaning)\n",
    "2. [Thorough EDA](#eda)\n",
    "3. [Building multiple linear regression model](#model-fitting)\n",
    "\n",
    "**Note**: this could be subjected to changes later after feedback from the ECON323 Instrutor's team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28983d56-2612-4c74-9436-f007e440bf20",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "For the data cleaning step, we will check and handle the missing values in the dataset. We will also identify categorical and continuous variables. For instance, the `CHAS` variable is a dummy variable indicating whether the tract bounds the Charles River or not, and is encoded as 0 or 1. Moreover, perform any special treatments toward outliers depending on the method that will be carried in the [model fitting phase](#model-fitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4880d72c-e5ed-4150-af2d-85ec545002f7",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "During the EDA phrase, we will conduct a thorough examination of the Boston Housing dataset. One of the key steps is to generate a correlation matrix, which can help us identify any potential issues related to multicollinearity between the independent variables. In addition, we will use side-by-side box plots to visualize the distributions of the continuous variables and detect any potential outliers or anomalies. Moreover, we will leverage other data visualization techniques, such as scatter plots and histograms, to better understand the relationships between the variables and explore potential trends or patterns in the data. Overall, the goal of EDA is to gain insights into the data and inform our subsequent modelling steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85fda3d-2402-42ac-9ce9-8c907b77f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  MEDV  \n",
       "0  396.90   4.98  24.0  \n",
       "1  396.90   9.14  21.6  \n",
       "2  392.83   4.03  34.7  \n",
       "3  394.63   2.94  33.4  \n",
       "4  396.90    NaN  36.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the dataset\n",
    "data_path = 'data/boston_housing_data.csv'\n",
    "boston = get_data(data_path=data_path)\n",
    "# check the first rows of the dataset\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6bad13-11b7-4fe9-a594-37637022415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>MEDV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>486.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.611874</td>\n",
       "      <td>11.211934</td>\n",
       "      <td>11.083992</td>\n",
       "      <td>0.069959</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.518519</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.715432</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.720192</td>\n",
       "      <td>23.388876</td>\n",
       "      <td>6.835896</td>\n",
       "      <td>0.255340</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>27.999513</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.155871</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.175000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>7.125000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.253715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>76.800000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.430000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.560263</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>93.975000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  486.000000  486.000000  486.000000  486.000000  506.000000  506.000000   \n",
       "mean     3.611874   11.211934   11.083992    0.069959    0.554695    6.284634   \n",
       "std      8.720192   23.388876    6.835896    0.255340    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081900    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.253715    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.560263   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  486.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.518519    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     27.999513    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.175000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     76.800000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     93.975000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT        MEDV  \n",
       "count  486.000000  506.000000  \n",
       "mean    12.715432   22.532806  \n",
       "std      7.155871    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      7.125000   17.025000  \n",
       "50%     11.430000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize the dataset \n",
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e457d9-4f2d-445e-b487-076107ec797a",
   "metadata": {},
   "source": [
    "According to the summary table above, we can see the basic statistical information for numerical data, including count, mean, standard deviation, minimum, 25% percentile, median, 75% percentile, and maximum. \n",
    "\n",
    "Certainly, there is one column in the dataset that requires special attention, which is the `CHAS` column. `CHAS` is a dummy variable that takes a value of 1 if the property is adjacent to the Charles River, and 0 otherwise. The `CHAS` are 0 for the 25th, 50th, and 75th percentiles, indicating that the distribution of this variable is skewed. To gain a deeper understanding of the data and identify any interesting trends or statistics, it may be beneficial to visualize the dataset through plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d640981-9bf3-46be-9a70-173d707f8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 1: Boxplot of Boston Housing Features', option = \"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a8e73-a764-4109-a069-32b83b232ed3",
   "metadata": {},
   "source": [
    "According to the Figure 1 above, we can observe that several columns in the Boston dataset such as `CRIM`, `ZN`, `RM`, and `B` appear to have many outliers. In the context of machine learning, extreme outliers can be problematic because they can skew statistical measures such as mean and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b122362-7357-42b5-8be0-efdbf9e3f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 2: Histogram of Boston Housing Features', option = \"hist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2024356-c17b-4c08-927e-77565e91c85d",
   "metadata": {},
   "source": [
    "In addition to identifying outliers, it is also important to understand the distributions of the individual columns in the dataset. The histograms generated earlier suggest that columns `CRIM`, `ZN`, `LSTAT`, `DIS` and `B` have skewed distributions. \n",
    "The distributions of the other columns appear to be normal or bimodal, except for `CHAS`, which is a discrete variable.\n",
    "In contrast, the histogram of `MEDV` (the variable we are trying to predict) appears to have a roughly normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03c69bc9-c837-44e4-adfa-ffd4e67c8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eda_plot(data = boston, title = 'Figure 3: Correlation Matrix Heatmap for Boston Dataset', option = \"heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff263da2-25bb-4232-a588-47a5aabaf9e6",
   "metadata": {},
   "source": [
    "Based on the Figure 3 above, it can be observed that there are several variables that exhibit a strong correlation with the target variable, `MEDV`. Specifically, the variable `RM` exhibits a correlation coefficient of $0.7$ with `MEDV`, while `LSTAT` exhibits a correlation coefficient of $0.74$ with `MEDV`. These correlations are relatively high compared to the other variables, and suggest that `RM` and `LSTAT` may be useful predictors for our regression model.\n",
    "\n",
    "However, it is also important to note that the variable `TAX` exhibits a strong correlation with the variable `RAD`, with a correlation coefficient of $0.91$. This suggests the presence of multicollinearity, which can cause issues in regression analysis such as inflated standard errors, reduced statistical power, and unstable coefficients. Therefore, it may be necessary to address multicollinearity in the modeling process, perhaps deleting one of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3c37b4-ea75-4e34-babe-82d8bfe1b545",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154827a-4a3c-49de-a1a8-72fb9320df95",
   "metadata": {},
   "source": [
    "In the model fitting phase, we will split the Boston Housing dataset into training and testing sets. We will then use the training set to select the relevant variables and build our final multiple linear regression model. The selection process can involve various techniques, such as stepwise regression or regularization, depending on the specific requirements of the project. Once we have the final model, we will use the testing set to evaluate its performance in terms of mean squared error (`MSE`). The goal is to ensure that the model can generalize well to new, unseen data and make accurate predictions. \n",
    "\n",
    "To further explore effects of using different methods of regression, we are going to fit multiple models to using similar metrics accross these to compare best fit of model, i.e. Bayesian Information Criterion (BIC) and Adjusted $R^2$ for inference (how well our model explains the effects of the explanatory variables is of the variable of interes); Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for prediction purposes models. Our ideal approach to test this is to fit the following models:\n",
    "1. Ordinary Least Squares (OLS) as baseline \n",
    "2. OLS with L1-norm regularization (Least Absolute Shrinkage and Selection Operator (LASSO Regression) )\n",
    "3. Random Forest Regression\n",
    "\n",
    "> Note: We are only interested in making predictions only, so this part of **Model Fitting** might need to refactor a bit\n",
    ">\n",
    "> Essentially get rid of Adjusted $R^2$ and BIC stuffs, we should only keep RMSE, MSE, MAE (could also be optional, but need to let me know beforehand to edit code correspondingly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ce80a-1ff7-45e6-9083-e5f5a2d850ef",
   "metadata": {},
   "source": [
    "We will ensure using same scoring metrics (defined below) across models, run a raw fit to all models to see how they perform against test set.  Then applying common preprocessing on all models and optimize hyperparameters for models, finally comparing the results again against the test set.\n",
    "\n",
    "> This is just an outline of what to do, It is just like a reminder, and could be better phrased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25328620-3308-440b-8c05-da254cd7e53f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load and Split data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc50397b-020e-4fec-8abc-a40fd81a3961",
   "metadata": {},
   "source": [
    "To begin, we will have a block to split the data into train and test data for further modelling purposes, such that all models would be using the same training set, and test set for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68b376a9-b0a7-4027-803c-c295306f42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data into X and y train and test portions\n",
    "X_train, X_test, y_train, y_test = split_data(data_path, proportion = 0.75, target = \"MEDV\", random_state=20230325)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3b0a7-e7a0-47a9-a320-39e38143a972",
   "metadata": {},
   "source": [
    "Moreover, we are going to use common scoring metrics (as mentioned earlier) across all models, so that we could compare their effectiveness in prediction:\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Mean Absolute Percentange Error (MAPE)\n",
    "- Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bed975f-0833-4bdf-997a-ba3129ccc8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define common scoring across models\n",
    "scoring = {\n",
    "    \"neg_rmse\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mape\": \"neg_mean_absolute_percentage_error\", \n",
    "    \"neg_mse\": \"neg_mean_squared_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabc9a4-fd6b-441e-a10a-cefb367f1edf",
   "metadata": {},
   "source": [
    "We could fit all the previous models mentioned at once and perform cross validation, in a more rapid way to compare and check which might perform better at **predicting house prices**.\n",
    "Then we could fit the methods individually in sections below raw, then with preprocessing, and check if getting similar scores like these CVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f1c4785-97c1-4ea8-a918-c93618df0c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ddf7e\">\n",
       "  <caption>Means CVs and +- std for Regression Methods</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ddf7e_level0_col0\" class=\"col_heading level0 col0\" >OLS</th>\n",
       "      <th id=\"T_ddf7e_level0_col1\" class=\"col_heading level0 col1\" >LASSO</th>\n",
       "      <th id=\"T_ddf7e_level0_col2\" class=\"col_heading level0 col2\" >Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row0\" class=\"row_heading level0 row0\" >fit_time</th>\n",
       "      <td id=\"T_ddf7e_row0_col0\" class=\"data row0 col0\" >0.002 (+/- 0.001)</td>\n",
       "      <td id=\"T_ddf7e_row0_col1\" class=\"data row0 col1\" >0.001 (+/- 0.001)</td>\n",
       "      <td id=\"T_ddf7e_row0_col2\" class=\"data row0 col2\" >0.215 (+/- 0.021)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row1\" class=\"row_heading level0 row1\" >score_time</th>\n",
       "      <td id=\"T_ddf7e_row1_col0\" class=\"data row1 col0\" >0.001 (+/- 0.001)</td>\n",
       "      <td id=\"T_ddf7e_row1_col1\" class=\"data row1 col1\" >0.001 (+/- 0.000)</td>\n",
       "      <td id=\"T_ddf7e_row1_col2\" class=\"data row1 col2\" >0.007 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row2\" class=\"row_heading level0 row2\" >test_rmse</th>\n",
       "      <td id=\"T_ddf7e_row2_col0\" class=\"data row2 col0\" >4.520 (+/- 0.788)</td>\n",
       "      <td id=\"T_ddf7e_row2_col1\" class=\"data row2 col1\" >5.003 (+/- 0.585)</td>\n",
       "      <td id=\"T_ddf7e_row2_col2\" class=\"data row2 col2\" >3.406 (+/- 0.530)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row3\" class=\"row_heading level0 row3\" >train_rmse</th>\n",
       "      <td id=\"T_ddf7e_row3_col0\" class=\"data row3 col0\" >4.278 (+/- 0.197)</td>\n",
       "      <td id=\"T_ddf7e_row3_col1\" class=\"data row3 col1\" >4.858 (+/- 0.167)</td>\n",
       "      <td id=\"T_ddf7e_row3_col2\" class=\"data row3 col2\" >1.337 (+/- 0.154)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row4\" class=\"row_heading level0 row4\" >test_mape</th>\n",
       "      <td id=\"T_ddf7e_row4_col0\" class=\"data row4 col0\" >0.163 (+/- 0.020)</td>\n",
       "      <td id=\"T_ddf7e_row4_col1\" class=\"data row4 col1\" >0.177 (+/- 0.023)</td>\n",
       "      <td id=\"T_ddf7e_row4_col2\" class=\"data row4 col2\" >0.123 (+/- 0.014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row5\" class=\"row_heading level0 row5\" >train_mape</th>\n",
       "      <td id=\"T_ddf7e_row5_col0\" class=\"data row5 col0\" >0.152 (+/- 0.006)</td>\n",
       "      <td id=\"T_ddf7e_row5_col1\" class=\"data row5 col1\" >0.168 (+/- 0.004)</td>\n",
       "      <td id=\"T_ddf7e_row5_col2\" class=\"data row5 col2\" >0.044 (+/- 0.002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row6\" class=\"row_heading level0 row6\" >test_mse</th>\n",
       "      <td id=\"T_ddf7e_row6_col0\" class=\"data row6 col0\" >20.925 (+/- 7.115)</td>\n",
       "      <td id=\"T_ddf7e_row6_col1\" class=\"data row6 col1\" >25.308 (+/- 5.804)</td>\n",
       "      <td id=\"T_ddf7e_row6_col2\" class=\"data row6 col2\" >11.826 (+/- 3.489)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ddf7e_level0_row7\" class=\"row_heading level0 row7\" >train_mse</th>\n",
       "      <td id=\"T_ddf7e_row7_col0\" class=\"data row7 col0\" >18.329 (+/- 1.684)</td>\n",
       "      <td id=\"T_ddf7e_row7_col1\" class=\"data row7 col1\" >23.627 (+/- 1.630)</td>\n",
       "      <td id=\"T_ddf7e_row7_col2\" class=\"data row7 col2\" >1.807 (+/- 0.408)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ad573fbf50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # fit all models at once to decide which one to use\n",
    "# # User-defined scoring to be hanlde in-terms of calculating scores for model\n",
    "# # Note scikit learn implemented these scorings in negative scale to minimize loss\n",
    "# # which we will scale back to positive behind the scenes\n",
    "\n",
    "def fit_all(scoring=scoring, **kwargs):\n",
    "    methods = [\"OLS\", \"LASSO\", \"Random Forest\"]\n",
    "    models = [mod for mod in [LinearRegression(), Lasso(), RandomForestRegressor()]]\n",
    "    # allocate space to store model fitted outputss\n",
    "    result_dict = {}\n",
    "    # fit all of these models at once\n",
    "    for method, model in zip(methods, models):\n",
    "        result_dict[method] = mean_std_cross_val_scores(model, \n",
    "                                                       X_train, \n",
    "                                                       y_train, \n",
    "                                                       return_train_score=True,\n",
    "                                                       scoring=scoring\n",
    "                                                     )\n",
    "    \n",
    "    result = pd.DataFrame(result_dict)\n",
    "    # rename the index to positive scale for easier comparison, replace the neg by empty\n",
    "    return result.rename(lambda x: x.replace(\"_neg\", \"\"))\n",
    "all_result = fit_all(X_train=X_train, y_train=y_train)\n",
    "all_result.style.set_caption(\"Means CVs and +- std for Regression Methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc3731-cefb-44a1-be74-23d22c983272",
   "metadata": {},
   "source": [
    "From above table, we could see fit time, score time, the scorings we set earlier **(MAPE, RMSE, MSE)**. And, each entry is the mean cross-validation (cv) scores $\\pm$ the standard deviation. \n",
    "Looking over test scores, random forest (RF) performs the best, this is expected since trees could model non-linear relationship (if existed) between variables. And it reduces variance (?) by bagging or bootstrap over random subsets of variables and observations. \n",
    "On the other hand, if looking at linear models, a plain OLS works better than a LASSO, this could be the reason that we only chose default $\\alpha$ for LASSO, hence it might not select that many variables. And by definition, OLS minimizes prediction error, hence it outperforms LASSO with default parameters\n",
    "For more information, we could also look at training scores, the tree model has lowest **train** score, which is also expected, since trees are tended to overfit if we set the depth to infinity or if not constrained. However, it takse longer fitting time due to its nature of bagging and ensemble of multiple regression trees.\n",
    "\n",
    "> I wrote this part by plain memory, some stuffs might not be accurate or true. Just like a general idea of what interpreation could be provide here @Tony"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2371b-5c0d-473b-bcae-78f23abde29d",
   "metadata": {},
   "source": [
    "With this \"cheating\" preview, we might have some sense on which models might work better for predicting housing prices in Boston. We hypothesize RF might the best model, and now we are going to verify or disprove it by fitting the models both raw and after preprocess data with optimized hyperparameters of the correpondent model. Of course, OLS would serve as our baseline model to compare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f773a-254b-4d48-bfce-767007ffe81c",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares (OLS)\n",
    "In the first stage of modeling, we will apply the traditional hedonic ordinary least squares regression to determine the coefficients between housing characteristics and housing prices. With the OLS approach, we can estimate the impact of each attribute on the market value of a heterogeneous good, and then we estimate prices in the form of willingness to pay for one unit of change of that characteristic.\n",
    "\n",
    "Now, we are going to fit a plain OLS regression model to act as our baseline model for comparing with other regression methods and see their improvements or weakness when applying regularization or boosting and bootstrapping. We propose a mixed model with different exponents for the explanatory variables as following form:\n",
    "\n",
    "$$P = \\beta_0 + \\beta_1 \\text{CRIM} + \\beta_2 \\text{ZN} +\\beta_3 \\text{INDUS}+\\beta_4 \\text{CHAS}+\\beta_5 \\text{NOX}+\\beta_6 \\text{RM}+\\beta_7 \\text{AGE}+\\beta_8 \\text{DIS}+\\beta_9 \\text{RAD}+\\beta_{10} \\text{TAX}+\\beta_{11} \\text{PTRATIO}+\\beta_{12} \\text{B}+\\beta_{13} \\text{LSTAT}+\\beta_{14} \\text{MEDV}$$\n",
    "\n",
    "whereas $P$ is the dependent variable `Price of housing unit in Boston`, and other $\\beta_i$ are beta estimates of independent variables $x_i \\quad \\forall i \\in [0, 14]$, and $\\beta_0$ is a special case, such it is the value of estimated $y$, where all the independent variables equal to 0.\n",
    "\n",
    "Hence, above equation can be generalized into matrix form below:\n",
    "\n",
    "$$Y = X\\beta + \\epsilon$$\n",
    "\n",
    "where $X$ is the design matrix with leading column of 1s (to represent the intercept term) and columns of independent variables $x_1, \\dots, x_{14}$, $\\beta$ is the matrix of all estimates from $\\beta_0, \\dots, \\beta_{14}$, and $\\epsilon$ is the  random error of measurements\n",
    "\n",
    "Then solving for $\\beta$ yields to the following:\n",
    "\n",
    "$$\\beta = (X^{T}X)^{-1}X^{T}Y$$\n",
    "\n",
    "Hence, we could use this above to solve for our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b009eea4-8779-4b68-8dc8-b10598a3b03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_73994\">\n",
       "  <caption>Test Scores on OLS Regression</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_73994_level0_col0\" class=\"col_heading level0 col0\" >RMSE</th>\n",
       "      <th id=\"T_73994_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_73994_level0_col2\" class=\"col_heading level0 col2\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_73994_level0_row0\" class=\"row_heading level0 row0\" >OLS</th>\n",
       "      <td id=\"T_73994_row0_col0\" class=\"data row0 col0\" >4.954000</td>\n",
       "      <td id=\"T_73994_row0_col1\" class=\"data row0 col1\" >24.539000</td>\n",
       "      <td id=\"T_73994_row0_col2\" class=\"data row0 col2\" >0.161000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ad2e019fd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call custom helper to fit OLS model and predict on test data, based on selected metrics\n",
    "raw_lr = fit_raw_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"OLS\")\n",
    "raw_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41be1b02-80aa-4479-a916-bbdc3def1104",
   "metadata": {},
   "source": [
    "> Add interpretation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc79f120-9c80-4aab-a369-68a57ffb2bf5",
   "metadata": {},
   "source": [
    "#### LASSO Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "502ae49e-b4ac-4273-b08a-1f7135bdaa22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_935da\">\n",
       "  <caption>Test Scores on LASSO Regression</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_935da_level0_col0\" class=\"col_heading level0 col0\" >RMSE</th>\n",
       "      <th id=\"T_935da_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_935da_level0_col2\" class=\"col_heading level0 col2\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_935da_level0_row0\" class=\"row_heading level0 row0\" >LASSO</th>\n",
       "      <td id=\"T_935da_row0_col0\" class=\"data row0 col0\" >6.407000</td>\n",
       "      <td id=\"T_935da_row0_col1\" class=\"data row0 col1\" >41.048000</td>\n",
       "      <td id=\"T_935da_row0_col2\" class=\"data row0 col2\" >0.171000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ad2e0184d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call custom helper to fit LASSO model and predict on test data, based on selected metrics\n",
    "raw_lasso = fit_raw_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"LASSO\")\n",
    "raw_lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4dc086-adbc-4b22-95b5-2267fa88f6b1",
   "metadata": {},
   "source": [
    "> Add interpretaion here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ac7987-bb1f-407a-952b-dfcd0c9ba493",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d2f5b-d6e5-4672-84b8-a1f2a475d76b",
   "metadata": {},
   "source": [
    "Other than using linear models, we are so using tree-like models, particularly ensembles of multiple decision regression trees which is a Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6404885-d253-4f75-88c0-f98f63d8b9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_71a2b\">\n",
       "  <caption>Test Scores on Random Forest Regression</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_71a2b_level0_col0\" class=\"col_heading level0 col0\" >RMSE</th>\n",
       "      <th id=\"T_71a2b_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_71a2b_level0_col2\" class=\"col_heading level0 col2\" >MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_71a2b_level0_row0\" class=\"row_heading level0 row0\" >Random Forest</th>\n",
       "      <td id=\"T_71a2b_row0_col0\" class=\"data row0 col0\" >3.768000</td>\n",
       "      <td id=\"T_71a2b_row0_col1\" class=\"data row0 col1\" >14.196000</td>\n",
       "      <td id=\"T_71a2b_row0_col2\" class=\"data row0 col2\" >0.119000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2ad5b9446d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# call custom helper to fit Random Forest model and predict on test data, based on selected metrics\n",
    "raw_rf = fit_raw_model(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, name=\"RF\")\n",
    "raw_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a69bdb-3ba6-4397-93b5-ce44dd48d98b",
   "metadata": {},
   "source": [
    "> Add interpretaion here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b931189-4ff9-4364-b9e7-6f80bfd6c0d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Division of Labor\n",
    "Based on the previous discussions, the team has divided the responsibilities as follows:\n",
    "\n",
    "- Tony: Coding\n",
    "- Wanxin: Coding and some textual descriptions\n",
    "- Xuan: Written section of the report\n",
    "\n",
    "However, the team may make adjustments to the division of labor as needed during the project to ensure that all tasks are completed efficiently and effectively. Effective communication and collaboration within the team will be critical to ensure that everyone is working together towards the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b5b37-1f71-4450-a8b7-bcf375b0932f",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Vishal, V. (2017, October 27). Boston Housing Dataset. Kaggle. Retrieved March 14, 2023, from https://www.kaggle.com/datasets/altavish/boston-housing-dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e77a0f-fbe7-4f6d-a573-e34b7d87798f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unused Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55d274a-bc74-4225-b4f1-0667af82607a",
   "metadata": {},
   "source": [
    "<!-- According to the summary table above, we can see the basic statistical information for numerical data, including count, mean, standard deviation, minimum, 25% percentile, median, 75% percentile, and maximum. \n",
    "\n",
    "Certainly, there is one column in the dataset that requires special attention, which is the CHAS column. CHAS is a dummy variable that takes a value of 1 if the property is adjacent to the Charles River, and 0 otherwise. The CHAS are 0 for the 25th, 50th, and 75th percentiles, indicating that the distribution of this variable is skewed. To gain a deeper understanding of the data and identify any interesting trends or statistics, it may be beneficial to visualize the dataset through plotting.### Model Fitting\n",
    "\n",
    "In the model fitting phase, we will split the Boston Housing dataset into training and testing sets. We will then use the training set to select the relevant variables and build our final multiple linear regression model. The selection process can involve various techniques, such as stepwise regression or regularization, depending on the specific requirements of the project. Once we have the final model, we will use the testing set to evaluate its performance in terms of mean squared error (`MSE`). The goal is to ensure that the model can generalize well to new, unseen data and make accurate predictions. \n",
    "\n",
    "To further explore effects of using different methods of regression, we are going to fit multiple models to using similar metrics accross these to compare best fit of model, i.e. Bayesian Information Criterion (BIC) and Adjusted $R^2$ for inference (how well our model explains the effects of the explanatory variables is of the variable of interes); Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) for prediction purposes models. Our ideal approach to test this is to fit the following models:\n",
    "1. Ordinary Least Squares (OLS) as baseline \n",
    "2. OLS with L2-norm regularization (Ridge Regression)\n",
    "3. OLS with L1-norm regularization (Least Absolute Shrinkage and Selection Operator (LASSO Regression) )\n",
    "4. Decision Tree Regression\n",
    "5. Random Forest Regression -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f6e7e1-2971-4a92-bae3-a59e4ca48a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the intercept and beta estimates\n",
    "# scikit-learn imported function\n",
    "# model = LinearRegression().fit(X_train, y_train)\n",
    "# self-defined function\n",
    "# intercept, estimates = OLS(X_train, y_train)\n",
    "# Use this option to NOT show scientific notation (default shows scientific notation)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# # print the estimates from self-defined func\n",
    "# print(f\"From self defined OLS, the intercept is {round(intercept,3)}, \\nAnd beta estimates are: \\n{np.ndarray.round(estimates, decimals=3)}\")\n",
    "\n",
    "# # prints the estimates from scikit-learn to inspect and compare\n",
    "# print(f\"\\nFrom scikit-learn function, the intercept is {round(model.intercept_, 3)}, \\nAnd beta estimates are: \")\n",
    "# print(np.ndarray.round(model.coef_[1:], decimals=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:econ323]",
   "language": "python",
   "name": "conda-env-econ323-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
